{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting training data from the ODC <img align=\"right\" src=\"../../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:** \n",
    "[s2_l2a](https://explorer.digitalearth.africa/s2_l2a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook will extract training data over Eastern Africa using geometries within a shapefile (or geojson). To do this, we rely on a custom `deafrica-sandbox-notebooks` function called `collect_training_data`, contained within the [deafrica_classificationtools](../Scripts/deafrica_classificationtools.py) script.\n",
    "\n",
    "1. Import, and preview our training data contained in the file: `'data/Eastern_training_data_20201204.geojson'`\n",
    "2. Extract training data from the datacube using a custom defined feature layer function that we can pass to `collect_training_data`. The training data function is stored in the python file `feature_layer_functions.py` - the functions are stored in a seperate file simply to keep this notebook tidy.\n",
    "\n",
    "    - **The features used to create the cropland mask are as follows:**\n",
    "        - For two seasons, January to June, and July to Decemeber:\n",
    "            - A geomedian composite of nine Sentinel-2 spectral bands\n",
    "            - Three measures of median absolute deviation\n",
    "            - NDVI, MNDWI, and LAI\n",
    "            - Cumulative Rainfall from CHIRPS\n",
    "            - Slope from SRTM (not seasonal, obviously)\n",
    "          \n",
    "          \n",
    "3. Seperate the coordinate values in the returned training data from step 2, and export the coordinates as a text file.\n",
    "4. Export the remaining training data (features other than coordinates) to disk as a text file for use in subsequent scripts\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=True, cloud_defaults=True)\n",
    "\n",
    "#import deafrica specific functions\n",
    "sys.path.append('../../Scripts')\n",
    "from deafrica_plotting import map_shapefile\n",
    "from deafrica_classificationtools import collect_training_data \n",
    "\n",
    "#import the custom feature layer functions\n",
    "from feature_layer_functions import gm_two_seasons_annual_mads\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `path`: The path to the input shapefile from which we will extract training data.\n",
    "* `field`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/Eastern_training_data_20201104.geojson' \n",
    "field = 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically find the number of cpus\n",
    "\n",
    "> **Note**: With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization, however, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 62\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "except:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & preview polygon data\n",
    "\n",
    "We can load and preview our input data shapefile using `geopandas`. The shapefile should contain a column with class labels (e.g. 'class'). These labels will be used to train our model. \n",
    "\n",
    "> Remember, the class labels **must** be represented by `integers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49666 -3.30737, 32.49693 -3.30716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49314 -3.30836, 32.49382 -3.30847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49962 -3.31316, 32.50028 -3.31338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.51721 -3.10441, 32.51716 -3.10465...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.38058 -2.69827, 32.38091 -2.69820...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                           geometry\n",
       "0      1  POLYGON ((32.49666 -3.30737, 32.49693 -3.30716...\n",
       "1      1  POLYGON ((32.49314 -3.30836, 32.49382 -3.30847...\n",
       "2      1  POLYGON ((32.49962 -3.31316, 32.50028 -3.31338...\n",
       "3      1  POLYGON ((32.51721 -3.10441, 32.51716 -3.10465...\n",
       "4      1  POLYGON ((32.38058 -2.69827, 32.38091 -2.69820..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input data shapefile\n",
    "input_data = gpd.read_file(path)\n",
    "\n",
    "# Plot first five rows\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data in an interactive map\n",
    "# map_shapefile(input_data, attribute=field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pass this shapefile to `collect_training_data`.  For each of the geometries in our shapefile we will extract features in accordance with the function `feature_layer_functions.gm_mads_two_seasons_training`. These will include:\n",
    "\n",
    "For two seasons, January to June, and July to Decemeber:\n",
    "- A geomedian composite of nine Sentinel-2 spectral bands\n",
    "- Three measures of median absolute deviation\n",
    "- NDVI, MNDWI, and LAI\n",
    "- Cumulative Rainfall from the CHIRPS\n",
    "- Slope from SRTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up a few extra inputs for `collect_training_data` and the datacube.  See the function docs [here](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/03b7b41d5f6526ff3f33618f7a0b48c0d10a155f/Scripts/deafrica_classificationtools.py#L650) for more information on these parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "zonal_stats = 'median'\n",
    "return_coords = True\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "products = ['s2_l2a']\n",
    "time = ('2019-01', '2019-12')\n",
    "measurements = [\n",
    "    'red', 'blue', 'green', 'nir', 'swir_1', 'swir_2', 'red_edge_1',\n",
    "    'red_edge_2', 'red_edge_3'\n",
    "]\n",
    "resolution = (-20, 20)\n",
    "output_crs = 'epsg:6933'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a new datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract training data\n",
    "\n",
    "> Remember, if running this function for the first time, its advisable to set `ncpus=1` to assist with debugging before triggering the parallelization (which won't return errors if something is not working correctly).  You can also limit the number of polygons to run for the first time by passing in `gdf=input_data[0:5]`, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing data using user supplied custom function\n",
      "Taking zonal statistic: median\n",
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 78/2880 [02:18<1:13:23,  1.57s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36LWQ_20190830_0_L2A/B03.tif\n",
      "  3%|▎         | 84/2880 [02:30<1:28:15,  1.89s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_36MVA_20190609_0_L2A/B11.tif\n",
      " 13%|█▎        | 364/2880 [07:36<51:08,  1.22s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NCB_20190807_0_L2A/B12.tif\n",
      " 14%|█▍        | 403/2880 [08:30<1:08:06,  1.65s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36MYD_20190512_0_L2A/B02.tif\n",
      " 14%|█▍        | 406/2880 [08:37<1:36:25,  2.34s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36MYD_20190512_0_L2A/B02.tif\n",
      " 14%|█▍        | 408/2880 [08:45<1:59:52,  2.91s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36MYD_20190512_0_L2A/B02.tif\n",
      " 14%|█▍        | 412/2880 [08:59<2:31:10,  3.68s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36MYD_20190512_0_L2A/B02.tif\n",
      " 16%|█▌        | 464/2880 [10:01<40:49,  1.01s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_36MYD_20190216_0_L2A/SCL.tif\n",
      " 16%|█▌        | 467/2880 [10:04<45:50,  1.14s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_36MZD_20190815_0_L2A/B03.tif\n",
      " 16%|█▋        | 471/2880 [10:13<1:22:22,  2.05s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36MYD_20190402_0_L2A/B03.tif\n",
      " 20%|█▉        | 570/2880 [11:51<43:56,  1.14s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36NXH_20190704_0_L2A/SCL.tif\n",
      " 26%|██▌       | 742/2880 [15:43<26:43,  1.33it/s]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36NXH_20191101_0_L2A/B02.tif\n",
      " 26%|██▋       | 757/2880 [15:58<38:31,  1.09s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37LBL_20190605_0_L2A/B06.tif\n",
      " 29%|██▉       | 828/2880 [16:35<13:09,  2.60it/s]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37LCL_20190625_1_L2A/B03.tif\n",
      " 30%|███       | 878/2880 [17:06<25:13,  1.32it/s]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37LCL_20190625_1_L2A/B03.tif\n",
      " 39%|███▉      | 1129/2880 [21:57<1:02:00,  2.12s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MBS_20190613_0_L2A/B12.tif\n",
      " 43%|████▎     | 1240/2880 [24:01<29:17,  1.07s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MBR_20190514_0_L2A/B06.tif\n",
      " 54%|█████▎    | 1541/2880 [29:45<09:58,  2.24it/s]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MBS_20190613_0_L2A/B12.tif\n",
      " 56%|█████▌    | 1601/2880 [30:57<14:49,  1.44it/s]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36NXF_20190122_0_L2A/B07.tif\n",
      " 56%|█████▌    | 1606/2880 [31:08<29:45,  1.40s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MBR_20190514_0_L2A/B06.tif\n",
      " 57%|█████▋    | 1647/2880 [31:49<12:56,  1.59it/s]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_36NXF_20190706_0_L2A/B12.tif\n",
      " 59%|█████▉    | 1713/2880 [33:10<33:11,  1.71s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NCJ_20190122_0_L2A/B06.tif\n",
      " 61%|██████    | 1750/2880 [34:03<33:03,  1.75s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PBN_20190122_0_L2A/B03.tif\n",
      " 64%|██████▍   | 1839/2880 [36:04<24:06,  1.39s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PGP_20191011_0_L2A/B06.tif\n",
      " 69%|██████▉   | 1991/2880 [39:41<19:00,  1.28s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCQ_20191230_0_L2A/B07.tif\n",
      " 70%|███████   | 2024/2880 [40:31<22:23,  1.57s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MDQ_20190101_0_L2A/SCL.tif\n",
      " 70%|███████   | 2026/2880 [40:34<19:57,  1.40s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MDQ_20190106_0_L2A/SCL.tif\n",
      " 70%|███████   | 2028/2880 [40:34<12:01,  1.18it/s]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PDP_20190117_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCP_20190606_0_L2A/B08.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PDP_20191113_0_L2A/B08.tif\n",
      " 83%|████████▎ | 2382/2880 [49:01<09:57,  1.20s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MDV_20190819_0_L2A/B05.tif\n",
      " 83%|████████▎ | 2383/2880 [49:02<08:44,  1.06s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCS_20190328_0_L2A/B07.tif\n",
      " 84%|████████▍ | 2414/2880 [49:47<09:52,  1.27s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MBT_20190419_0_L2A/B11.tif\n",
      " 96%|█████████▋| 2774/2880 [59:37<02:16,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 1 = 12.71 %\n",
      "Recollecting samples that failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/366 [00:00<?, ?it/s]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_35MRV_20190103_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190101_0_L2A/B12.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCM_20190117_0_L2A/B04.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190101_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBJ_20190117_0_L2A/B08.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEU_20190106_0_L2A/B03.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBA_20190223_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MDU_20190119_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBJ_20190117_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190116_0_L2A/B02.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFU_20190106_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGU_20190918_0_L2A/B03.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PCR_20190112_0_L2A/B07.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCM_20190107_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190111_0_L2A/SCL.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PEK_20190104_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NCJ_20190412_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NEJ_20190114_0_L2A/B07.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PDM_20190626_0_L2A/B07.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MCV_20190109_0_L2A/B08.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190124_0_L2A/B08.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PBP_20190117_0_L2A/B07.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NEJ_20190114_0_L2A/B07.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_35MQT_20190113_0_L2A/SCL.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PCQ_20190102_0_L2A/B12.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MDU_20190114_0_L2A/B12.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190101_0_L2A/B12.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCM_20190117_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBJ_20190107_0_L2A/B11.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBJ_20190117_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190124_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFU_20190116_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NBG_20190112_0_L2A/SCL.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBA_20190104_0_L2A/B05.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PDP_20190122_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190713_0_L2A/B08.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEV_20190116_0_L2A/B05.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGU_20190101_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190327_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PCQ_20190112_0_L2A/B08.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MCV_20190109_0_L2A/B03.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEU_20190116_0_L2A/B03.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MCV_20190119_0_L2A/B07.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190121_0_L2A/B07.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGU_20190111_0_L2A/B04.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFT_20190126_1_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGV_20190710_0_L2A/SCL.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBA_20190124_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_35MRV_20190113_0_L2A/B08.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NCJ_20190901_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCR_20190107_0_L2A/B11.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190104_0_L2A/SCL.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCQ_20190107_0_L2A/B12.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MEU_20190111_0_L2A/B02.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBJ_20190117_0_L2A/B05.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NCJ_20190901_0_L2A/SCL.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFT_20190106_0_L2A/SCL.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190101_0_L2A/B11.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20191001_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190111_0_L2A/B07.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEV_20190116_0_L2A/B02.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCQ_20190107_0_L2A/B03.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/B02.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFU_20190116_0_L2A/B11.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NBA_20190129_0_L2A/B02.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFU_20190116_0_L2A/B12.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_36MYD_20190107_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCQ_20190107_0_L2A/B04.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190116_0_L2A/B08.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190111_0_L2A/B07.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/B08.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBG_20190117_0_L2A/B07.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190713_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NCJ_20190701_0_L2A/SCL.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFU_20191112_1_L2A/B03.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/SCL.tif\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PDP_20190122_0_L2A/B07.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFT_20190106_0_L2A/B03.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NCJ_20190901_0_L2A/B07.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEU_20190116_0_L2A/B02.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37NBA_20190104_0_L2A/B04.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190327_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGU_20190111_0_L2A/B06.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190104_0_L2A/B07.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEU_20190116_0_L2A/B05.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NBA_20190119_0_L2A/B02.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGV_20190710_0_L2A/B12.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFT_20190106_0_L2A/B08.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PCM_20190112_0_L2A/B08.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/B04.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190114_0_L2A/B11.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGU_20190121_0_L2A/B08.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190124_0_L2A/B02.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190111_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGV_20191212_0_L2A/B11.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MEV_20190111_0_L2A/B02.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PDP_20190112_0_L2A/B04.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCL_20190117_0_L2A/B06.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MEV_20190101_0_L2A/SCL.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/B02.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCL_20190117_0_L2A/B08.tif\n",
      "\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190822_0_L2A/B07.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PDP_20190112_0_L2A/B08.tif\n",
      "\n",
      "\n",
      "  0%|          | 1/366 [00:45<4:37:42, 45.65s/it]sentinel-s2-l2a-cogs/2019/S2B_37MCV_20190124_0_L2A/SCL.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MCV_20190119_0_L2A/SCL.tif\n",
      "  1%|          | 2/366 [00:45<3:14:08, 32.00s/it]\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEV_20190116_0_L2A/B08.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEU_20190116_0_L2A/SCL.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/B03.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36MYD_20190512_0_L2A/B04.tif\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGV_20190101_0_L2A/B05.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCL_20190318_0_L2A/B11.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MEU_20190116_0_L2A/B08.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGV_20191212_0_L2A/B07.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGU_20190121_0_L2A/B12.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MEV_20190111_0_L2A/B11.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGU_20190106_0_L2A/B03.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PCL_20190112_0_L2A/B11.tif\n",
      "\n",
      "\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCL_20190107_0_L2A/SCL.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCL_20190117_0_L2A/B07.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PCL_20190122_0_L2A/B02.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PCL_20190117_0_L2A/B11.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37PCL_20190112_0_L2A/B06.tif\n",
      "\n",
      "  1%|          | 4/366 [00:48<2:17:41, 22.82s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFU_20190116_0_L2A/B11.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MGV_20190101_0_L2A/B04.tif\n",
      "\n",
      "  1%|▏         | 5/366 [00:49<1:38:02, 16.29s/it]\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGV_20190106_0_L2A/B04.tifError opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37MFU_20190101_0_L2A/B11.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGV_20190106_0_L2A/B06.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MGV_20191212_0_L2A/B02.tif\n",
      "Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MFU_20190106_0_L2A/B06.tif\n",
      "\n",
      " 20%|█▉        | 73/366 [02:38<01:50,  2.65it/s]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37LBL_20190119_0_L2A/B05.tif\n",
      " 21%|██        | 76/366 [02:48<09:52,  2.04s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37LBL_20190119_0_L2A/B05.tif\n",
      " 45%|████▍     | 163/366 [04:46<05:46,  1.70s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37NCA_20190320_0_L2A/B12.tif\n",
      " 51%|█████▏    | 188/366 [05:22<04:13,  1.43s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_37PDM_20190626_0_L2A/B07.tif\n",
      " 75%|███████▌  | 276/366 [08:37<02:48,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 2 = 0.9 %\n",
      "Removed 29 rows wth NaNs &/or Infs\n",
      "Output shape:  (2655, 34)\n",
      "CPU times: user 3min 38s, sys: 15 s, total: 3min 53s\n",
      "Wall time: 1h 8min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "column_names, model_input = collect_training_data(\n",
    "                                    gdf=input_data,\n",
    "                                    products=products,\n",
    "                                    dc_query=query,\n",
    "                                    ncpus=25,\n",
    "                                    return_coords=return_coords,\n",
    "                                    field=field,\n",
    "                                    zonal_stats=zonal_stats,\n",
    "                                    custom_func=gm_two_seasons_annual_mads,\n",
    "                                    fail_threshold=0.015,\n",
    "                                    max_retries=4\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class', 'red_S1', 'blue_S1', 'green_S1', 'nir_S1', 'swir_1_S1', 'swir_2_S1', 'red_edge_1_S1', 'red_edge_2_S1', 'red_edge_3_S1', 'NDVI_S1', 'LAI_S1', 'MNDWI_S1', 'rain_S1', 'red_S2', 'blue_S2', 'green_S2', 'nir_S2', 'swir_1_S2', 'swir_2_S2', 'red_edge_1_S2', 'red_edge_2_S2', 'red_edge_3_S2', 'NDVI_S2', 'LAI_S2', 'MNDWI_S2', 'rain_S2', 'edev', 'sdev', 'bcdev', 'slope', 'x_coord', 'y_coord']\n",
      "\n",
      "[[      1.         0.06       0.05 ...       3.17 3135170.   -421860.  ]\n",
      " [      1.         0.09       0.06 ...       2.95 3135520.   -421710.  ]\n",
      " [      1.         0.13       0.08 ...       3.   3137450.   -395860.  ]\n",
      " ...\n",
      " [      1.         0.15       0.07 ...      21.67 3675590.   1611350.  ]\n",
      " [      0.         0.21       0.07 ...       7.39 4390610.    915810.  ]\n",
      " [      0.         0.25       0.09 ...       5.5  4368090.    802450.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(column_names)\n",
    "print('')\n",
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate the coordinates\n",
    "\n",
    "By setting `return_coords=True` in the `collect_training_data` function, our training data now has two extra columns called `x_coord` and `y_coord`.  We need to seperate these from our training dataset as they will not be used to train the machine learning model. Instead, these variables will be used to help conduct Spatial K-fold Cross validation (SKVC) in the notebook `3_Train_fit_evaluate_classifier`.  For more information on why this is important, see this [article](https://www.tandfonline.com/doi/abs/10.1080/13658816.2017.1346255?journalCode=tgis20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_filename = \"results/training_data/training_data_coordinates_20201104.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_variables = ['x_coord', 'y_coord']\n",
    "model_col_indices = [column_names.index(var_name) for var_name in coord_variables]\n",
    "\n",
    "np.savetxt(coordinates_filename, model_input[:, model_col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Once we've collected all the training data we require, we can write the data to disk. This will allow us to import the data in the next step(s) of the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = \"results/training_data/gm_two_seasons_annual_mads_training_data_20201512.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all columns except the x-y coords\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[0:-2]]\n",
    "#Export files to disk\n",
    "np.savetxt(output_file, model_input[:, model_col_indices], header=\" \".join(column_names[0:-2]), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To continue working through the notebooks in this `Eastern Africa Cropland Mask` workflow, go to the next notebook `2_Inspect_training_data.ipynb`.\n",
    "\n",
    "1. **Extracting_training_data (this notebook)** \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
