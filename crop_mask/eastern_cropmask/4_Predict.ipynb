{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction <img align=\"right\" src=\"../../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Using the model we created in the `3_Train_fit_evaluate_classifier.ipynb`, this notebook will make predictions on new data to generate a cropland mask for Eastern Africa. The notebook will ceate both pixel-wise classifications and classification probabilities. Results are saved to disk as Cloud-Optimised-Geotiffs.\n",
    "\n",
    "1. Open and inspect the shapefile which delineates the extent we're classifying\n",
    "2. Import the model\n",
    "3. Make predictions on new data loaded through the ODC.  The pixel classification will also undergo a post-processing step where steep slopes and water are masked using a SRTM derivative and WOfS, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/env/lib/python3.6/site-packages/pkg_resources/__init__.py:1135: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  self, resource_name\n",
      "/env/lib/python3.6/site-packages/branca/colormap.py:29: ResourceWarning: unclosed file <_io.BufferedReader name='/env/lib/python3.6/site-packages/branca/_cnames.json'>\n",
      "  resource_package, resource_path_cnames).read().decode()\n",
      "/env/lib/python3.6/site-packages/branca/colormap.py:33: ResourceWarning: unclosed file <_io.BufferedReader name='/env/lib/python3.6/site-packages/branca/_schemes.json'>\n",
      "  resource_package, resource_path_schemes).read().decode()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import subprocess as sp\n",
    "from joblib import load\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "sys.path.append('../../Scripts')\n",
    "from deafrica_datahandling import load_ard\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "from deafrica_classificationtools import HiddenPrints, predict_xr\n",
    "from deafrica_plotting import map_shapefile\n",
    "\n",
    "#import out feature layer function for prediction\n",
    "from feature_layer_functions import gm_two_seasons_annual_mads\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from deafrica_spatialtools import xr_rasterize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell. You can use the dashboard to monitor the progress of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38865</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/eefaye.chong/proxy/8787/status' target='_blank'>/user/eefaye.chong/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>31</li>\n",
       "  <li><b>Memory: </b>254.70 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38865' processes=1 threads=31, memory=254.70 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `model_path`: The path to the location where the model exported from the previous notebook is stored\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `test_shapefile`: A shapefile containing polygons that represent regions where you want to test your model. The shapefile should have a unique identifier as this will be used to export classification results to disk as geotiffs. Alternatively, this could be a shapefile that defines the extent of the entire AOI you want to classify.\n",
    "* `results`: A folder location to store the classified geotiffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results/gm_two_seasons_annual_mads_training_data_20201512.joblib'\n",
    "\n",
    "training_data = \"results/training_data/gm_two_seasons_annual_mads_training_data_20201512.txt\"\n",
    "\n",
    "test_shapefile = 'data/eastern_testing_sites.geojson'\n",
    "\n",
    "results = 'results/classifications/'\n",
    "\n",
    "model_type='gm_two_seasons_annual_mads'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and inspect test_shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[1.7501522238596312, 35.899982256111464], controls=(ZoomControl(options=['position', 'zoom_in_text'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf = gpd.read_file(test_shapefile)\n",
    "\n",
    "# gdf.head()\n",
    "map_shapefile(gdf, attribute='GRID_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the model\n",
    "\n",
    "The code below will also re-open the training data we exported from `2_Inspect_training_data.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up datacube query\n",
    "\n",
    "These query options should exactl match the query params in `1_Extract_training_data.ipynb`, unless there are measurements that no longer need to be loaded because they were dropped during a feature selection process (which we didn't conduct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products = ['s2_l2a']\n",
    "time = ('2019-01', '2019-12')\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "measurements = [\n",
    "    'red', 'blue', 'green', 'nir', 'swir_1', 'swir_2', 'red_edge_1',\n",
    "    'red_edge_2', 'red_edge_3'\n",
    "]\n",
    "resolution = (-50, 50)\n",
    "output_crs = 'epsg:6933'\n",
    "dask_chunks = {'x':1500,'y':1500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through test tiles and predict\n",
    "\n",
    "For every tile we list in the `test_shapefile`, we calculate the feature layers, and then use the DE Africa function `predict_xr` to classify the data.\n",
    "\n",
    "The results are exported to file as Cloud-Optimised Geotiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.red.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on grid: E-20\n",
      "((1500, 1500, 764), (1500, 1500, 289), 12)\n",
      "(0, 0, 3)\n",
      "dask.array<tmads-84fefad3, shape=(3764, 3289, 36), dtype=float32, chunksize=(1500, 1500, 12), chunktype=numpy.ndarray>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'There are no fields in dtype float32.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/dev/fork/deafrica-sandbox-notebooks/crop_mask/eastern_cropmask/feature_layer_functions.py\u001b[0m in \u001b[0;36mgm_two_seasons_annual_mads\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mannual_tmad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr_tmad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# write separate tmad function that calls on Kirill's geomedian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mannual_tmad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sdev'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannual_tmad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sdev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0mannual_tmad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bcdev'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannual_tmad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bcdev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mannual_tmad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edev'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannual_tmad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/dask/array/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1557\u001b[0m         ):\n\u001b[1;32m   1558\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1559\u001b[0;31m                 \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1560\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_sliced_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'There are no fields in dtype float32.'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, row in gdf[:1].iterrows():\n",
    "    \n",
    "    #get id for labelling\n",
    "    g_id=gdf.iloc[index]['GRID_ID']\n",
    "    print('working on grid: ' + g_id)\n",
    "    \n",
    "    # Get the geometry\n",
    "    geom = geometry.Geometry(geom=gdf.iloc[index].geometry,\n",
    "                             crs=gdf.crs)\n",
    "\n",
    "     # generate a datacube query object\n",
    "    query = {\n",
    "        'time': time,\n",
    "        'measurements': measurements,\n",
    "        'resolution': resolution,\n",
    "        'output_crs': output_crs,\n",
    "        'group_by' : 'solar_day',\n",
    "    }\n",
    "    \n",
    "    # Update dc query with geometry      \n",
    "    query.update({'geopolygon': geom}) \n",
    "    \n",
    "    #load data -\n",
    "#     data = xr.open_dataset(results+'input/Eastern_tile_'+g_id+'_inputs.nc').chunk(dask_chunks)\n",
    "    \n",
    "    with HiddenPrints():\n",
    "        ds = load_ard(dc=dc,\n",
    "                      products=products,\n",
    "                      dask_chunks=dask_chunks,\n",
    "                      **query)\n",
    "\n",
    "    #calculate features\n",
    "    data = gm_two_seasons_annual_mads(ds)\n",
    "    \n",
    "#    print(data)\n",
    "    \n",
    "    # predict using the imported model\n",
    "#     predicted = predict_xr(model,\n",
    "#                            data,\n",
    "# #                            proba=True,\n",
    "# #                           persist=True,\n",
    "#                            clean=True,\n",
    "# #                           return_input=True\n",
    "#                           ).compute()\n",
    "    \n",
    "    \n",
    "#     # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "# #     with HiddenPrints():\n",
    "# #         mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "# #         predicted = predicted.where(mask)\n",
    "    \n",
    "#     #grab just the predictions for post process filtering\n",
    "#     predict = predicted.Predictions\n",
    "    \n",
    "#     #mask with WOFS\n",
    "#     wofs_query=query.pop('measurements')\n",
    "#     wofs=dc.load(product='ga_ls8c_wofs_2_summary',**query)\n",
    "#     wofs = wofs.frequency > 0.2 # threshold\n",
    "#     predict=predict.where(~wofs, 0)    \n",
    "    \n",
    "#     #mask steep slopes\n",
    "#     url_slope = \"https://deafrica-data.s3.amazonaws.com/ancillary/dem-derivatives/cog_slope_africa.tif\"\n",
    "#     slope = rio_slurp_xarray(url_slope, gbox=ds.geobox)\n",
    "#     slope = slope > 35\n",
    "#     predict=predict.where(~slope, 0)\n",
    "#     predict=predict.astype(np.int16)\n",
    "    \n",
    "#     #export classifications to disk\n",
    "#     write_cog(predict, results+'predicted/Eastern_tile_'+g_id+'_prediction_pixel_'+model_type+'_20201204.tif', overwrite=True)\n",
    "# #     write_cog(predicted.Probabilities, results+ 'proba/Eastern_tile_'+g_id+'_probabilities_pixel_'+model_type+'_20201204.tif', overwrite=True)\n",
    "    \n",
    "#     #remove predict and proba so we can export the inputs\n",
    "# #     del predicted['Probabilities']\n",
    "#     del predicted['Predictions']\n",
    "    \n",
    "#     #export inputs\n",
    "#     predicted.to_netcdf(results+ 'input/Eastern_tile_'+g_id+'_inputs.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predicted.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.Predictions.plot(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odc.algo import randomize, reshape_for_geomedian\n",
    "from feature_layer_functions import xr_tmad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = dc.load(product='ga_s2_gm', like=ds, measurements = [\n",
    "                'red', 'blue', 'green', 'nir', 'swir_1', 'swir_2', 'red_edge_1',\n",
    "                'red_edge_2', 'red_edge_3'], dask_chunks=dask_chunks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = reshape_for_geomedian(gm)\n",
    "gm = gm.squeeze().astype(np.float32).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm = da.from_array(gm, chunks=(750, 750,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1500, 1500, 764), (1500, 1500, 289), 12)\n",
      "(0, 0, 3)\n",
      "dask.array<tmads-c3fef1a3, shape=(3764, 3289, 36), dtype=float32, chunksize=(1500, 1500, 12), chunktype=numpy.ndarray>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "t=xr_tmad(ds, gm).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['sdev'] = -np.log(t['sdev'])\n",
    "t['bcdev'] = -np.log(t['bcdev'])\n",
    "t['edev'] = -np.log(t['edev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sdev.plot(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict_xr(model,\n",
    "                       data,\n",
    "#                       proba=True,\n",
    "#                       persist=True,\n",
    "                       clean=True,\n",
    "#                       return_input=True\n",
    "                      )#.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To continue working through the notebooks in this `Eastern Africa Cropland Mask` workflow, go to the next notebook `5_Object-based_filtering.ipynb`.\n",
    "\n",
    "1. [Extracting_training_data](1_Extracting_training_data.ipynb) \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. [Train_fit_evaluate_classifier](3_Train_fit_evaluate_classifier.ipynb)\n",
    "4. **Predict (this notebook)**\n",
    "5. [Object-based_filtering](5_Object-based_filtering.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
