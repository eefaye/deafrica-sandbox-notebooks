{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "1. Generate predictions using out imported model at a number of test locations.\n",
    "2. Plot the results of our test locations (assuming the test locations aren't too large).\n",
    "3. Generate a large-scale classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://packages.dea.ga.gov.au/hdstats/hdstats-0.1.5.tar.gz\n",
    "# !pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/env/lib/python3.6/site-packages/cligj/__init__.py:17: FutureWarning: cligj 1.0.0 will require Python >= 3.7\n",
      "  warn(\"cligj 1.0.0 will require Python >= 3.7\", FutureWarning)\n",
      "/env/lib/python3.6/site-packages/pkg_resources/__init__.py:1135: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  self, resource_name\n",
      "/env/lib/python3.6/site-packages/branca/colormap.py:29: ResourceWarning: unclosed file <_io.BufferedReader name='/env/lib/python3.6/site-packages/branca/_cnames.json'>\n",
      "  resource_package, resource_path_cnames).read().decode()\n",
      "/env/lib/python3.6/site-packages/branca/colormap.py:33: ResourceWarning: unclosed file <_io.BufferedReader name='/env/lib/python3.6/site-packages/branca/_schemes.json'>\n",
      "  resource_package, resource_path_schemes).read().decode()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import subprocess as sp\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from odc.algo import xr_geomedian, int_geomedian\n",
    "\n",
    "sys.path.append('../../Scripts')\n",
    "from deafrica_datahandling import load_ard\n",
    "from deafrica_bandindices import calculate_indices\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "from deafrica_classificationtools import HiddenPrints\n",
    "from deafrica_plotting import rgb, display_map, map_shapefile\n",
    "from deafrica_classificationtools import HiddenPrints, predict_xr\n",
    "from deafrica_spatialtools import xr_rasterize, xr_vectorize\n",
    "\n",
    "from feature_layer_functions import gm_mads_three_seasons\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell. You can use the dashboard to monitor the progress of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:44649</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/eefaye.chong/proxy/8787/status' target='_blank'>/user/eefaye.chong/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>31</li>\n",
       "  <li><b>Memory: </b>254.70 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:44649' processes=1 threads=31, memory=254.70 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `model_path`: The path to the location where the model exported from the previous notebook is stored\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `features_scaled`: \n",
    "* `sc_path`: Use this parameter to indicate whether or not the features where scaled using the `standardScalar()` method in the previous notebook. If you did scale the features then provide the path to the `standardScalar` values output in the previous notebook. Otherwise, set this parameters to `None`\n",
    "* `test_shapefile`: A shapefile containing polygons that represent regions where you want to test your model. The shapefile should have a unique identifier as this will be used to export classification results to disk as geotiffs.\n",
    "* `results`: A folder location to store the classified geotiffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results/gm_mads_three_seasons_ml_model.joblib'\n",
    "\n",
    "training_data = \"results/training_data/gm_mads_three_seasons_training_data_allfls.txt\"\n",
    "\n",
    "sc_path = None #'results/std_scaler.bin'\n",
    "\n",
    "test_shapefile = 'data/eastern_testing_sites.geojson'\n",
    "\n",
    "results = 'results/classifications/'\n",
    "\n",
    "model_type='gm_mads_three_seasons'\n",
    "# custom function in feature_layer_functions.py now needs to have\n",
    "# Dask chunks commented back in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and inspect test_shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d8be906fe84fedb8cd05d400ce062d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff00eaf61174ddcbe690adfb02847fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[1.7501522238596312, 35.899982256111464], controls=(ZoomControl(options=['position', 'zoom_in_text'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf = gpd.read_file(test_shapefile)\n",
    "\n",
    "# gdf.head()\n",
    "map_shapefile(gdf, attribute='GRID_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the model\n",
    "\n",
    "If we ran the optional feature scaling method in the `3_Train_fit_evaluate_classifier.ipynb`, then we will also load in the standard scalar values.\n",
    "\n",
    "The code below will also re-open the training data we exported from `2_Inspect_training_data.ipynb` and grab the column names (features we selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red_S1', 'blue_S1', 'green_S1', 'nir_S1', 'swir_1_S1', 'swir_2_S1', 'red_edge_1_S1', 'red_edge_2_S1', 'red_edge_3_S1', 'edev_S1', 'sdev_S1', 'bcdev_S1', 'NDVI_S1', 'LAI_S1', 'MNDWI_S1', 'rain_S1', 'red_S2', 'blue_S2', 'green_S2', 'nir_S2', 'swir_1_S2', 'swir_2_S2', 'red_edge_1_S2', 'red_edge_2_S2', 'red_edge_3_S2', 'edev_S2', 'sdev_S2', 'bcdev_S2', 'NDVI_S2', 'LAI_S2', 'MNDWI_S2', 'rain_S2', 'red_S3', 'blue_S3', 'green_S3', 'nir_S3', 'swir_1_S3', 'swir_2_S3', 'red_edge_1_S3', 'red_edge_2_S3', 'red_edge_3_S3', 'edev_S3', 'sdev_S3', 'bcdev_S3', 'NDVI_S3', 'LAI_S3', 'MNDWI_S3', 'rain_S3', 'slope']\n"
     ]
    }
   ],
   "source": [
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[2:]\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up datacube query\n",
    "\n",
    "These query options should match the query params in `1_Extract_training_data.ipynb`, unless there are measurements that no longer need to be loaded because they were dropped during the feature selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products =  ['s2_l2a']\n",
    "time = ('2019-01','2019-12')\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "measurements =  ['red','blue','green','nir','swir_1','swir_2','red_edge_1','red_edge_2', 'red_edge_3']\n",
    "resolution = (-20,20)\n",
    "output_crs='epsg:6933'\n",
    "dask_chunks={'x':2000,'y':2000,'time':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through test tiles and predict\n",
    "\n",
    "For every tile we list in the `test_shapefile`, we calculate the feature layers, and then use the DE Africa function `predict_xr` to classify the data.\n",
    "\n",
    "The results are exported to file as Cloud-Optimised Geotiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on grid: B-19\n",
      "   predicting...\n",
      "working on grid: D-17\n",
      "   predicting...\n",
      "working on grid: D-15\n",
      "   predicting...\n",
      "working on grid: G-15\n",
      "   predicting...\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for index, row in gdf[1:5].iterrows():\n",
    "    \n",
    "    #get id for labelling\n",
    "    g_id=gdf.iloc[index]['GRID_ID']\n",
    "    print('working on grid: ' + g_id)\n",
    "    \n",
    "    # Get the geometry\n",
    "    geom = geometry.Geometry(row.geometry.__geo_interface__,\n",
    "                             geometry.CRS(f'EPSG:{gdf.crs.to_epsg()}'))\n",
    "\n",
    "     # generate a datacube query object\n",
    "    query = {\n",
    "        'time': time,\n",
    "        'measurements': measurements,\n",
    "        'resolution': resolution,\n",
    "        'output_crs': output_crs,\n",
    "        'group_by' : 'solar_day',\n",
    "    }\n",
    "    \n",
    "    # Update dc query with geometry      \n",
    "    query.update({'geopolygon': geom}) \n",
    "    \n",
    "    #load data\n",
    "    with HiddenPrints():\n",
    "        ds = load_ard(dc=dc,\n",
    "                      products=products,\n",
    "                      dask_chunks=dask_chunks,\n",
    "                      **query)\n",
    "\n",
    "    #calculate features\n",
    "    data = gm_mads_three_seasons(ds)\n",
    "    #predict using the imported model\n",
    "    predicted = predict_xr(model,\n",
    "                           data,\n",
    "                           #proba=True,\n",
    "#                            persist=True,\n",
    "                           clean=True,\n",
    "#                            return_input=True\n",
    "                          ).compute()\n",
    "    \n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "#     with HiddenPrints():\n",
    "#         mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "#         predicted = predicted.where(mask)\n",
    "    \n",
    "    #mask with WOFS\n",
    "    wofs_query=query.pop('measurements')\n",
    "    wofs=dc.load(product='ga_ls8c_wofs_2_summary',**query)\n",
    "    wofs = wofs.frequency > 0.2 # threshold\n",
    "    out=predicted.Predictions.where(~wofs, 0)    \n",
    "    \n",
    "    #export classifications to disk\n",
    "    write_cog(out, results+ 'Eastern_tile_'+g_id+'_prediction_pixel_'+model_type+'_20201111.tif', overwrite=True)\n",
    "    \n",
    "    #export layer for running image segmentation\n",
    "#     write_cog(predicted.NDVI_S1, results+ 'Eastern_tile_'+g_id+'_NDVI_S1.tif', overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# x=xr.open_rasterio(results+ 'Eastern_tile_'+'E-20'+'_prediction_pixel_'+model_type+'_20201106.tif').squeeze()\n",
    "# x.plot(figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.48pm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
